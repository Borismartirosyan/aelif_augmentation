{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa49a19-37ef-454b-82bf-793509179f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/backpack/',\n",
    "    ):\n",
    "\n",
    "    titles = [\"Noise Conv\", \"Mask\", \"Original\", \"Train Data\"]  # Titles for each image\n",
    "    noise_conv_augmented = [f'{noise_conv_augmented_path}{file}' for file in sorted(os.listdir(noise_conv_augmented_path))]\n",
    "    mask_augmented = [f'{mask_augmented_path}{file}' for file in sorted(os.listdir(mask_augmented_path))]\n",
    "    original_augmented = [f'{original_augmented_path}{file}' for file in sorted(os.listdir(original_augmented_path))]\n",
    "    train_data = [f'{train_data_path}{file}' for file in sorted(os.listdir(train_data_path))]\n",
    "    \n",
    "    print(len(noise_conv_augmented), len(mask_augmented), len(original_augmented), len(train_data))\n",
    "\n",
    "    for i in range(len(noise_conv_augmented)):\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        grid = ImageGrid(fig, 111,\n",
    "            nrows_ncols=(1, 4),  # Creates 1 row, 4 columns\n",
    "            axes_pad=0.3,  # Pad between Axes in inches\n",
    "        )\n",
    "\n",
    "        im1 = Image.open(noise_conv_augmented[i])\n",
    "        im2 = Image.open(mask_augmented[i])\n",
    "        im3 = Image.open(original_augmented[i])\n",
    "        im4 = Image.open(train_data[i])\n",
    "\n",
    "        images = [im1, im2, im3, im4]  # Store images in a list\n",
    "\n",
    "        for ax, im, title in zip(grid, images, titles):\n",
    "            ax.imshow(im)\n",
    "            ax.set_title(title, fontsize=14)  # Set the title for each subplot\n",
    "            ax.axis(\"off\")  # Hide axes for better visualization\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/backpack/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7b349b-d553-4025-abbc-f5c55e2925b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/candle/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/candle/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/candle/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/candle/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd40b69-5d06-49e4-bdeb-5f585f1143e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/clock/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/clock/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/clock/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/clock/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934918fa-202b-4ba0-b906-bdb2ae14994e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/cat/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/cat/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/cat/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/cat/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28e517-5850-49ad-977c-1558680a0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/colorful_sneaker/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/colorful_sneaker/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/colorful_sneaker/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/colorful_sneaker/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807e2b67-6dcb-44cb-bced-dabc73188914",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog_data/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog_data/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog_data/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/dog_data/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a4657-8a65-4d3a-b43d-d056788dae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog2/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog2/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog2/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/dog2/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f94ed7-eb77-49bf-90b4-a721a3ce319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog3/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog3/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog3/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/dog3/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14739a09-110d-4886-b183-c860ad96a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/teapot/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/teapot/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/teapot/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/teapot/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedafca-a993-47fe-bc4c-a3aa5135eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_grid(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/vase/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/vase/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/vase/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/vase/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1072cedf-ee0c-40f3-8443-1507303fb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb29921-26b7-4f87-aa59-99fdb88cfef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from scipy.stats import wasserstein_distance, wasserstein_distance_nd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compare_image_embeds(\n",
    "    noise_conv_augmented_path,\n",
    "    mask_augmented_path,\n",
    "    original_augmented_path,\n",
    "    train_data_path,\n",
    "    model=model, \n",
    "    preprocess=preprocess, \n",
    "    device='cuda'\n",
    "):\n",
    "    if model is None or preprocess is None:\n",
    "        raise ValueError(\"Model and preprocess function must be provided\")\n",
    "\n",
    "    noise_conv_embeds = []\n",
    "    mask_embeds = []\n",
    "    orig_embeds = []\n",
    "    train_embeds = []\n",
    "    \n",
    "    for img_name in tqdm(os.listdir(noise_conv_augmented_path), desc=\"Processing noise_conv images\"):\n",
    "        img = Image.open(os.path.join(noise_conv_augmented_path, img_name))\n",
    "        img_emb = model.encode_image(preprocess(img).unsqueeze(0).to(device))\n",
    "        noise_conv_embeds.append(img_emb.detach().cpu())  # Fixed\n",
    "\n",
    "    for img_name in tqdm(os.listdir(mask_augmented_path), desc=\"Processing mask images\"):\n",
    "        img = Image.open(os.path.join(mask_augmented_path, img_name))\n",
    "        img_emb = model.encode_image(preprocess(img).unsqueeze(0).to(device))\n",
    "        mask_embeds.append(img_emb.detach().cpu())  # Fixed\n",
    "\n",
    "    for img_name in tqdm(os.listdir(original_augmented_path), desc=\"Processing original images\"):\n",
    "        img = Image.open(os.path.join(original_augmented_path, img_name))\n",
    "        img_emb = model.encode_image(preprocess(img).unsqueeze(0).to(device))\n",
    "        orig_embeds.append(img_emb.detach().cpu())  # Fixed\n",
    "\n",
    "    for img_name in tqdm(os.listdir(train_data_path), desc=\"Processing train images\"):\n",
    "        img = Image.open(os.path.join(train_data_path, img_name))\n",
    "        img_emb = model.encode_image(preprocess(img).unsqueeze(0).to(device))\n",
    "        train_embeds.append(img_emb.detach().cpu())  # Fixed\n",
    "\n",
    "    # Ensure tensors are stacked correctly\n",
    "    noise_conv_embeds = torch.stack(noise_conv_embeds)\n",
    "    mask_embeds = torch.stack(mask_embeds)\n",
    "    orig_embeds = torch.stack(orig_embeds)\n",
    "    train_embeds = torch.stack(train_embeds)\n",
    "    \n",
    "    train_embeds = train_embeds.squeeze(1)        # was (6,1,768), becomes (6,768)\n",
    "    noise_conv_embeds = noise_conv_embeds.squeeze(1)\n",
    "    mask_embeds = mask_embeds.squeeze(1)\n",
    "    orig_embeds = orig_embeds.squeeze(1)\n",
    "\n",
    "    \n",
    "    print(noise_conv_embeds)\n",
    "    # Compute Wasserstein distances\n",
    "    noise_conv_train = wasserstein_distance_nd(train_embeds.numpy(), noise_conv_embeds.numpy())\n",
    "    mask_train = wasserstein_distance_nd(train_embeds.numpy(), mask_embeds.numpy())\n",
    "    orig_train = wasserstein_distance_nd(train_embeds.numpy(), orig_embeds.numpy())\n",
    "\n",
    "    print(f'distance between noise_conv and train data : {noise_conv_train}, distance between mask and train data : {mask_train}, distance between orig dreambooth and train data : {orig_train}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92679ae-a312-4174-8443-5b7a5b6ff1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 6/6 [00:05<00:00,  1.15it/s]\n",
      "Processing mask images: 100%|██████████| 6/6 [00:00<00:00, 10.98it/s]\n",
      "Processing original images: 100%|██████████| 6/6 [00:00<00:00,  8.47it/s]\n",
      "Processing train images: 100%|██████████| 6/6 [00:00<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0492,  0.2352,  0.2517,  ...,  0.1484, -0.4138,  0.3000],\n",
      "        [ 0.0481,  0.3728,  0.0120,  ...,  0.1597, -0.1814, -0.0070],\n",
      "        [ 0.1653,  0.5352,  0.0043,  ...,  0.2018, -0.2399, -0.0677],\n",
      "        [-0.0395,  0.6665,  0.1575,  ..., -0.0351, -0.3289, -0.0373],\n",
      "        [-0.1361,  0.4922, -0.0010,  ...,  0.1742, -0.3669,  0.1549],\n",
      "        [ 0.0323,  0.5220,  0.0292,  ...,  0.0247, -0.0648, -0.1471]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 5.422525166600748, distance between mask and train data : 5.190588970844959, distance between orig dreambooth and train data : 5.659513114773426\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/backpack/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7c12fb-e34b-4be1-85a5-baee97222679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 5/5 [00:00<00:00,  9.01it/s]\n",
      "Processing mask images: 100%|██████████| 5/5 [00:00<00:00,  6.67it/s]\n",
      "Processing original images: 100%|██████████| 5/5 [00:00<00:00,  7.41it/s]\n",
      "Processing train images: 100%|██████████| 5/5 [00:00<00:00, 10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0485,  0.1002,  0.1814,  ...,  0.1432, -0.4604,  0.2576],\n",
      "        [ 0.3188,  0.3252, -0.2074,  ..., -0.1685, -0.0271, -0.1335],\n",
      "        [-0.2062,  0.1270,  0.1340,  ..., -0.0497, -0.2404, -0.0219],\n",
      "        [ 0.0473,  0.2676, -0.2625,  ...,  0.3562, -0.1370,  0.1158],\n",
      "        [ 0.0271,  0.1256,  0.0828,  ..., -0.0698, -0.1610,  0.5854]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 8.168058760352741, distance between mask and train data : 7.428080172948528, distance between orig dreambooth and train data : 7.15294789309725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack_dog/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack_dog/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/backpack_dog/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/backpack_dog/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d31d0e-df4d-4ea2-963d-4a905183d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 5/5 [00:00<00:00,  8.67it/s]\n",
      "Processing mask images: 100%|██████████| 5/5 [00:00<00:00,  9.29it/s]\n",
      "Processing original images: 100%|██████████| 5/5 [00:00<00:00,  9.06it/s]\n",
      "Processing train images: 100%|██████████| 5/5 [00:00<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2496,  0.0690,  0.2712,  ...,  0.2297, -0.0436, -0.1190],\n",
      "        [-0.3103,  0.0586,  0.3320,  ...,  0.1890,  0.1564,  0.2084],\n",
      "        [-0.3674,  0.0209,  0.2544,  ...,  0.3608,  0.1235,  0.2468],\n",
      "        [-0.2695, -0.1940, -0.0052,  ...,  0.2190,  0.0642, -0.0428],\n",
      "        [-0.3335, -0.1791,  0.1537,  ...,  0.4163,  0.2112,  0.0400]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 8.260203430739429, distance between mask and train data : 7.25416150471224, distance between orig dreambooth and train data : 7.151987293451321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/candle/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/candle/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/candle/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/candle/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c126989-ecc3-40fd-b9f8-f22e87c76045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 5/5 [00:00<00:00,  9.72it/s]\n",
      "Processing mask images: 100%|██████████| 5/5 [00:00<00:00,  8.90it/s]\n",
      "Processing original images: 100%|██████████| 5/5 [00:00<00:00, 10.66it/s]\n",
      "Processing train images: 100%|██████████| 5/5 [00:00<00:00, 20.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3787,  0.1713,  0.2847,  ...,  0.6606, -0.1177, -0.3037],\n",
      "        [-0.1675,  0.1287,  0.3542,  ...,  0.3936,  0.0481, -0.2214],\n",
      "        [-0.1321,  0.0607,  0.2783,  ...,  0.2179, -0.0580, -0.0872],\n",
      "        [-0.0071, -0.1566,  0.1608,  ...,  0.5342, -0.1458,  0.0470],\n",
      "        [-0.1667,  0.0071,  0.2485,  ...,  0.6401, -0.0897, -0.2451]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 4.084605939897933, distance between mask and train data : 3.8363398822721693, distance between orig dreambooth and train data : 3.8333657693737018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/cat/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/cat/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/cat/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/cat/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e00b3e6-3cc5-4ed5-88bf-b099f15cb732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 6/6 [00:00<00:00,  8.45it/s]\n",
      "Processing mask images: 100%|██████████| 6/6 [00:00<00:00,  8.20it/s]\n",
      "Processing original images: 100%|██████████| 6/6 [00:00<00:00, 10.43it/s]\n",
      "Processing train images: 100%|██████████| 6/6 [00:00<00:00, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2104,  0.0364, -0.1140,  ...,  0.5605, -0.0102,  0.2388],\n",
      "        [ 0.2002, -0.0634, -0.2705,  ...,  0.5845,  0.0249,  0.2186],\n",
      "        [ 0.2233, -0.0782, -0.2893,  ...,  0.8311, -0.1056, -0.0104],\n",
      "        [ 0.1787,  0.2913, -0.2585,  ...,  0.3167, -0.0622,  0.1938],\n",
      "        [ 0.1962, -0.0182, -0.2177,  ...,  0.5278,  0.0865, -0.0216],\n",
      "        [ 0.1497,  0.1044, -0.1951,  ...,  0.4319, -0.2407,  0.0247]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 5.891908305708153, distance between mask and train data : 5.602047164894623, distance between orig dreambooth and train data : 5.087403861110392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/clock/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/clock/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/clock/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/clock/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b4efe4-99e8-472e-9886-505c04102ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 5/5 [00:00<00:00,  9.26it/s]\n",
      "Processing mask images: 100%|██████████| 5/5 [00:00<00:00, 10.54it/s]\n",
      "Processing original images: 100%|██████████| 5/5 [00:00<00:00, 10.91it/s]\n",
      "Processing train images: 100%|██████████| 5/5 [00:00<00:00,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7344, -0.0427,  0.3933,  ..., -0.3684,  0.2559,  0.7080],\n",
      "        [-0.6841, -0.0433,  0.2874,  ...,  0.0101, -0.0152,  0.7773],\n",
      "        [-0.5513,  0.0436,  0.4504,  ..., -0.1003,  0.1686,  0.5854],\n",
      "        [-0.4680, -0.0260,  0.3879,  ..., -0.0847,  0.2489,  0.5713],\n",
      "        [-0.7456, -0.1243,  0.3203,  ..., -0.1539,  0.1337,  0.5244]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 5.204299973406551, distance between mask and train data : 5.31989876510594, distance between orig dreambooth and train data : 5.44148438737909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/colorful_sneaker/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/colorful_sneaker/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/colorful_sneaker/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/colorful_sneaker/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f38198-0032-44c0-8c7a-a5b23d9deee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 5/5 [00:00<00:00, 10.42it/s]\n",
      "Processing mask images: 100%|██████████| 5/5 [00:00<00:00,  9.94it/s]\n",
      "Processing original images: 100%|██████████| 5/5 [00:00<00:00, 11.52it/s]\n",
      "Processing train images: 100%|██████████| 5/5 [00:00<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1030,  0.3682, -0.0508,  ...,  0.4941, -0.2197, -0.0889],\n",
      "        [-0.1215,  0.1979,  0.0604,  ...,  0.3982, -0.2474, -0.0775],\n",
      "        [-0.0326,  0.2566, -0.0432,  ...,  0.3882, -0.1984, -0.1196],\n",
      "        [-0.0214,  0.3037,  0.1014,  ...,  0.4514, -0.2100, -0.1973],\n",
      "        [-0.0535,  0.4026, -0.0590,  ...,  0.3997, -0.2800, -0.0518]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 3.409234934484566, distance between mask and train data : 4.093245945449982, distance between orig dreambooth and train data : 4.124167961837731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog_data/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog_data/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog_data/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/dog_data/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9483dfe3-9a98-4489-8bf0-dc36d7b2315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 6/6 [00:00<00:00,  8.63it/s]\n",
      "Processing mask images: 100%|██████████| 6/6 [00:00<00:00,  9.67it/s]\n",
      "Processing original images: 100%|██████████| 6/6 [00:00<00:00,  9.44it/s]\n",
      "Processing train images: 100%|██████████| 6/6 [00:00<00:00, 14.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1785,  0.0429,  0.0091,  ...,  0.4309,  0.2163, -0.3662],\n",
      "        [ 0.2554,  0.1664, -0.0050,  ...,  0.2350,  0.3765, -0.2583],\n",
      "        [ 0.1461,  0.2283,  0.1247,  ...,  0.5474,  0.2058, -0.1831],\n",
      "        [-0.0155,  0.2249,  0.0861,  ...,  0.3867,  0.2422, -0.1137],\n",
      "        [ 0.1382,  0.2089, -0.0856,  ...,  0.5049,  0.1624, -0.1219],\n",
      "        [ 0.3140,  0.1554,  0.0413,  ...,  0.3865,  0.2788, -0.1672]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 4.544684845961004, distance between mask and train data : 4.543222441615668, distance between orig dreambooth and train data : 4.675732601286677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog2/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog2/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog2/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/dog2/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce7ec91-be8f-4766-a3cb-15aeb40d4d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 6/6 [00:00<00:00, 10.37it/s]\n",
      "Processing mask images: 100%|██████████| 6/6 [00:00<00:00,  8.90it/s]\n",
      "Processing original images: 100%|██████████| 6/6 [00:00<00:00,  9.37it/s]\n",
      "Processing train images: 100%|██████████| 6/6 [00:00<00:00, 21.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0421, -0.4739, -0.1809,  ...,  0.2512,  0.1219,  0.1768],\n",
      "        [ 0.0014, -0.2249, -0.1547,  ...,  0.2966, -0.1580,  0.0295],\n",
      "        [ 0.3105, -0.0655,  0.1011,  ...,  0.3621,  0.3506,  0.3430],\n",
      "        [-0.0016, -0.1791, -0.0551,  ...,  0.1416,  0.0411,  0.4504],\n",
      "        [ 0.2014, -0.2030, -0.0798,  ...,  0.2445,  0.0390,  0.2390],\n",
      "        [ 0.0580, -0.2698, -0.1422,  ...,  0.4636,  0.2822,  0.1583]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 6.656507657089268, distance between mask and train data : 5.961517583448776, distance between orig dreambooth and train data : 6.199716276806934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog3/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog3/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/dog3/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/dog3/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d50ffb95-c13f-4b68-b72f-745a1592b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 5/5 [00:00<00:00, 10.60it/s]\n",
      "Processing mask images: 100%|██████████| 5/5 [00:00<00:00,  8.19it/s]\n",
      "Processing original images: 100%|██████████| 5/5 [00:00<00:00, 10.11it/s]\n",
      "Processing train images: 100%|██████████| 5/5 [00:00<00:00, 21.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0869,  0.6987,  0.1025,  ...,  0.1915, -0.5171, -0.5093],\n",
      "        [ 0.1887,  0.5400, -0.0015,  ...,  0.3345, -0.3359, -0.4006],\n",
      "        [ 0.1114,  0.5933,  0.1057,  ...,  0.2490, -0.4639, -0.3174],\n",
      "        [-0.2642,  0.3599,  0.0418,  ...,  0.3628, -0.3743, -0.6045],\n",
      "        [ 0.2607,  0.5195,  0.0082,  ...,  0.1066, -0.4517, -0.3301]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 5.697302285103021, distance between mask and train data : 5.425620640038461, distance between orig dreambooth and train data : 5.470084692752099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/teapot/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/teapot/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/teapot/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/teapot/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147c4a24-c42e-447c-b7d9-f9995ef8eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing noise_conv images: 100%|██████████| 6/6 [00:00<00:00, 11.22it/s]\n",
      "Processing mask images: 100%|██████████| 6/6 [00:00<00:00,  8.63it/s]\n",
      "Processing original images: 100%|██████████| 6/6 [00:00<00:00, 13.28it/s]\n",
      "Processing train images: 100%|██████████| 6/6 [00:00<00:00, 36.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3154,  0.3669,  0.3733,  ...,  0.1754,  0.2532, -0.2974],\n",
      "        [-0.2051,  0.5068,  0.2607,  ...,  0.4790, -0.0849, -0.0417],\n",
      "        [ 0.2771,  0.4343,  0.0109,  ...,  0.2542,  0.3054,  0.1175],\n",
      "        [-0.0194,  0.4666,  0.0081,  ...,  0.2900, -0.1204,  0.0194],\n",
      "        [ 0.0922,  0.4399, -0.0030,  ...,  0.4585,  0.0147,  0.3040],\n",
      "        [-0.1241,  0.4092,  0.0773,  ...,  0.3218,  0.0334,  0.1746]],\n",
      "       dtype=torch.float16)\n",
      "distance between noise_conv and train data : 5.261885310450992, distance between mask and train data : 4.807509642387186, distance between orig dreambooth and train data : 4.996196860592633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_image_embeds(\n",
    "    noise_conv_augmented_path = '/home/jupyter/aelif_augmentation/results_images/vase/aug/',\n",
    "    mask_augmented_path = '/home/jupyter/aelif_augmentation/results_images/vase/aug-mask/',\n",
    "    original_augmented_path = '/home/jupyter/aelif_augmentation/results_images/vase/original/',\n",
    "    train_data_path = '/home/jupyter/aelif_augmentation/dreambooth/dataset/vase/'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": ".m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m126"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
